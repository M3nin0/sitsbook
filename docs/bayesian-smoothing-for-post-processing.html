<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Bayesian smoothing for post-processing | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes</title>
<meta name="author" content="Gilberto Camara">
<meta name="author" content="Rolf Simoes">
<meta name="author" content="Felipe Souza">
<meta name="author" content="Charlotte Pelletier">
<meta name="author" content="Alber Sanchez">
<meta name="author" content="Pedro R. Andrade">
<meta name="author" content="Karine Ferreira">
<meta name="author" content="Gilberto Queiroz">
<meta name="description" content="Introduction Machine learning algorithms rely on training samples that are derived from “pure” pixels, hand-picked by users to represent the desired output classes. Given the presence of mixed...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Bayesian smoothing for post-processing | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/cover_sits_book.png">
<meta property="og:description" content="Introduction Machine learning algorithms rely on training samples that are derived from “pure” pixels, hand-picked by users to represent the desired output classes. Given the presence of mixed...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian smoothing for post-processing | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta name="twitter:description" content="Introduction Machine learning algorithms rely on training samples that are derived from “pure” pixels, hand-picked by users to represent the desired output classes. Given the presence of mixed...">
<meta name="twitter:image" content="/images/cover_sits_book.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/IBM_Plex_Serif-0.4.7/font.css" rel="stylesheet">
<link href="libs/IBM_Plex_Mono-0.4.7/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title=""><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="setup.html">Setup</a></li>
<li><a class="" href="acknowledgements.html">Acknowledgements</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li><a class="" href="earth-observation-data-cubes.html">Earth observation data cubes</a></li>
<li><a class="" href="operations-on-data-cubes.html">Operations on data cubes</a></li>
<li><a class="" href="working-with-time-series.html">Working with time series</a></li>
<li><a class="" href="improving-the-quality-of-training-samples.html">Improving the quality of training samples</a></li>
<li><a class="" href="machine-learning-for-data-cubes.html">Machine learning for data cubes</a></li>
<li><a class="" href="image-classification-in-data-cubes.html">Image classification in data cubes</a></li>
<li><a class="active" href="bayesian-smoothing-for-post-processing.html">Bayesian smoothing for post-processing</a></li>
<li><a class="" href="validation-and-accuracy-measurements.html">Validation and accuracy measurements</a></li>
<li><a class="" href="uncertainty-and-active-learning.html">Uncertainty and active learning</a></li>
<li><a class="" href="ensemble-prediction-from-multiple-models.html">Ensemble prediction from multiple models</a></li>
<li><a class="" href="object-based-time-series-image-analysis.html">Object-based time series image analysis</a></li>
<li><a class="" href="technical-annex.html">Technical Annex</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-smoothing-for-post-processing" class="section level1 unnumbered">
<h1>Bayesian smoothing for post-processing<a class="anchor" aria-label="anchor" href="#bayesian-smoothing-for-post-processing"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-1" class="section level2 unnumbered">
<h2>Introduction<a class="anchor" aria-label="anchor" href="#introduction-1"><i class="fas fa-link"></i></a>
</h2>
<p>Machine learning algorithms rely on training samples that are derived from “pure” pixels, hand-picked by users to represent the desired output classes. Given the presence of mixed pixels in images regardless of resolution, and the considerable data variability within each class, these classifiers often produce results with outliers or misclassified pixels. Therefore, post-processing techniques have become crucial to refine the labels of a classified image . Post-processing methods reduce salt-and-pepper and border effects, where single pixels or small groups of pixels are classified differently from their larger surrounding areas; these effects leads to visual discontinuity and inconsistency. By mitigating these errors and minimizing noise, post-processing improves the quality of the initial classification results, bringing a significant gain in the overall accuracy and interpretability of the final output .</p>
<p>The <code>sits</code> package uses a <em>time-first, space-later</em> approach. Since machine learning classifiers in <code>sits</code> are mostly pixel-based, it is necessary to complement them with spatial smoothing methods. These methods improve the accuracy of land classification by incorporating spatial and contextual information into the classification process. The smoothing method available in <code>sits</code> uses an Empirical Bayes approach, adjusted to the specific properties of land classification. The assumption is that class probabilities at the local level should be similar and provide the baseline for comparison with the pixel values produced by the classifier. Based on these two elements, Bayesian smoothing adjusts the probabilities for the pixels, considering spatial dependence.</p>
</div>
<div id="empirical-bayesian-estimation" class="section level2 unnumbered">
<h2>Empirical Bayesian estimation<a class="anchor" aria-label="anchor" href="#empirical-bayesian-estimation"><i class="fas fa-link"></i></a>
</h2>
<p>The Bayesian estimate is based on the probabilities produced by the classifiers. Let <span class="math inline">\(p_{i,k} \geq 0\)</span> be the prior probability of the <span class="math inline">\(i\)</span>-th pixel belonging to class <span class="math inline">\(k \in \{1, \ldots, m\}\)</span>. The probabilities <span class="math inline">\(p_{i,k}\)</span> are the classifier’s output, being subject to noise, outliers, and classification errors. Our estimation aims to remove these effects and obtain values that approximate the actual class probability better.</p>
<p>We convert the class probability values <span class="math inline">\(p_{i,k}\)</span> to log-odds values using the logit function, to transform probability values ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> to values from negative infinity to infinity. The conversion from probabilities logit values is helpful to support our assumption of normal distribution for our data.</p>
<p><span class="math display">\[
    x_{i,k} = \ln \left(\frac{p_{i,k}}{1 - p_{i,k}}\right)
\]</span></p>
<p>The standard Empirical Bayesian updating leads to the estimation values for posterior distribution which can be expressed as a weighted mean</p>
<p><span class="math display">\[
{E}[\mu_{i,k} | x_{i,k}] =
\Biggl [ \frac{s^2_{i,k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times
x_{i,k} +
\Biggl [ \frac{\sigma^2_{k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times m_{i,k},
\]</span>
where:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(x_{i,k}\)</span> is the logit value for pixel <span class="math inline">\(i\)</span> and class <span class="math inline">\(k\)</span>.</li>
<li>
<span class="math inline">\(m_{i,k}\)</span> is the average of logit values for pixels of class <span class="math inline">\(k\)</span>
in the neighborhood of pixel <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(s^2_{i,k}\)</span> is the variance of logit values for pixels of class <span class="math inline">\(k\)</span>
in the neighborhood of pixel <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(\sigma^2_{k}\)</span> is an user-derived hyperparameter which estimates the variance for class <span class="math inline">\(k\)</span>, expressed in logits.</li>
</ol>
<p>The above equation is a weighted average between the value <span class="math inline">\(x_{i,k}\)</span> for the pixel and the mean <span class="math inline">\(m_{i,k}\)</span> for the neighboring pixels. When the variance <span class="math inline">\(s^2_{i,k}\)</span> for the neighbors is too high, the algorithm gives more weight to the pixel value <span class="math inline">\(x_{i,k}\)</span>. When class variance <span class="math inline">\(\sigma^2_k\)</span> increases, the results gives more weight to the neighborhood mean <span class="math inline">\(m_{i,k}\)</span>.</p>
</div>
<div id="using-non-isotropic-neighbourhoods" class="section level2 unnumbered">
<h2>Using non-isotropic neighbourhoods<a class="anchor" aria-label="anchor" href="#using-non-isotropic-neighbourhoods"><i class="fas fa-link"></i></a>
</h2>
<p>The fundamental idea behind Bayesian smoothing for land classification is that individual pixels area related to those close to it. Each pixel usually has the same class as most of its neighbours. These closeness relations are expressed in similar values of class probability. If we find a pixel assigned to “Water” surrounded by pixels labelled as “Forest”, such pixel may have been wrongly labelled. To check if the pixel has been mislabelled, we look at the class probabilities for the pixels and its neighbors. There are possible situations:</p>
<ol style="list-style-type: decimal">
<li><p>The outlier has a class probability distribution very different from its neighbors. For example, its probability for belonging to the “Water” class is 80% while that of being a “Forest” is 20%. If we also consider that “Water” pixels have a smaller variance, since water areas have a strong signal in multispectral images, our post-processing method will not change the pixel’s label.</p></li>
<li><p>The outlier has a class probability distribution similar from its neighbors. Consider a case where a pixel has a 47% probability for “Water” and 43% probability for “Forest”. This small difference indicates that we need to look at the neighbourhood to improve the information produced by the classifier. In these cases, the post-processing estimate may change the pixel’s label.</p></li>
</ol>
<p>Pixels in the border between two areas of different classes pose a challenge. Only some of their neighbours belong to the same class as the pixel. To address this issue, we employ a non-isotropic definition of a neighbourhood to estimate the prior class distribution. For instance, consider a boundary pixel with a neighborhood defined by a 7 x 7 window, located along the border between Forest and Grassland classes. To estimate the prior probability of the pixel being labelled as a Forest, we should only take into account the neighbours on one side of the border that are likely to be correctly classified as Forest. Pixels on the opposite side of the border should be disregarded, since they are unlikely to belong to the same spatial process. In practice, we use only half of the pixels in the 7 x 7 window, opting for those that have a higher probability of being named as Forest. For the prior probability of the Grassland class, we reverse the selection and only consider those on the opposite side of the border.</p>
<p>Although this choice of neighbourhood may seem unconventional, it is consistent with the assumption of non-continuity of the spatial processes describing each class. A dense forest patch, for example, will have pixels with strong spatial autocorrelation for values of the Forest class; however, this spatial autocorrelation doesn’t extend across its border with other land classes.</p>
</div>
<div id="effect-of-the-hyperparameter" class="section level2 unnumbered">
<h2>Effect of the hyperparameter<a class="anchor" aria-label="anchor" href="#effect-of-the-hyperparameter"><i class="fas fa-link"></i></a>
</h2>
<p>The parameter <span class="math inline">\(\sigma^2_k\)</span> controls the level of smoothness. If <span class="math inline">\(\sigma^2_k\)</span> is zero, the value <span class="math inline">\({E}[\mu_{i,k} | x_{i,k}]\)</span> will be equal to the pixel value <span class="math inline">\(x_{i,k}\)</span>. The parameter <span class="math inline">\(\sigma^2_k\)</span> expresses confidence in the inherent variability of the distribution of values of a class <span class="math inline">\(k\)</span>. The smaller the parameter <span class="math inline">\(\sigma^2_k\)</span>, the more we trust the estimated probability values produced by the classifier for class <span class="math inline">\(k\)</span>. Conversely, higher values of <span class="math inline">\(\sigma^2_k\)</span> indicate lower confidence in the classifier outputs and improved confidence in the local averages.</p>
<p>Consider the following two-class example. Take a pixel with probability <span class="math inline">\(0.4\)</span> (logit <span class="math inline">\(x_{i,1} = -0.4054\)</span>) for class A and probability <span class="math inline">\(0.6\)</span> (logit <span class="math inline">\(x_{i,2} = 0.4054\)</span>) for class B. Without post-processing, the pixel will be labeled as class B. Consider that the local average is <span class="math inline">\(0.6\)</span> (logit <span class="math inline">\(m_{i,1} = 0.4054\)</span>) for class A and <span class="math inline">\(0.4\)</span> (logit <span class="math inline">\(m_{i,2} = -0.4054\)</span>) for class B. This is a case of an outlier classified originally as class B in the midst of a set of class A pixels.</p>
<p>Given this situation, we apply the proposed method. Suppose the local variance of logits to be <span class="math inline">\(s^2_{i,1} = 5\)</span> for class A and <span class="math inline">\(s^2_{i,2} = 10\)</span> and for class B. This difference is to be expected if the local variability of class A is smaller than that of class B. To complete the estimate, we need to set the parameter <span class="math inline">\(\sigma^2_{k}\)</span>, representing our belief in the variability of the probability values for each class.</p>
<p>Setting <span class="math inline">\(\sigma^2_{k}\)</span> will be based on our confidence in the local variability of each class around pixel <span class="math inline">\({i}\)</span>. If we considered the local variability to be high, we can take both <span class="math inline">\(\sigma^2_1\)</span> for class A and <span class="math inline">\(\sigma^2_2\)</span> for class B to be both 10. In this case, the Bayesian estimated probability for class A is <span class="math inline">\(0.52\)</span> and for class B is <span class="math inline">\(0.48\)</span> and the pixel will be relabelled as being class A.</p>
<p>By contrast, if we consider local variability to be high If we set <span class="math inline">\(\sigma^2\)</span> to be 5 for both classes A and B, the Bayesian probability estimate will be <span class="math inline">\(0.48\)</span> for class A and <span class="math inline">\(0.52\)</span> for class B. In this case, the original class will be kept. Therefore, the result is sensitive to the subjective choice of the hyperparameter.</p>
<p>We make the following recommendations for setting the <span class="math inline">\(\sigma^2_{k}\)</span> parameter:</p>
<ol style="list-style-type: decimal">
<li><p>Set the <span class="math inline">\(\sigma^2_{k}\)</span> parameter with high values (<span class="math inline">\(20\)</span> or above) to increase the neighborhood influence compared with the probability values for each pixel. Classes whose probabilities have strong spatial autocorrelation will tend to replace outliers of different classes.</p></li>
<li><p>Set the <span class="math inline">\(\sigma^2_{k}\)</span> parameter with low values (<span class="math inline">\(5\)</span> or below) to reduce the neighborhood influence compared with the probabilities for each pixel of class <span class="math inline">\(k\)</span>. In this way, classes with low spatial autocorrelation are more likely not to be relabeled.</p></li>
</ol>
<p>Consider the case of forest areas and watersheds. If an expert wishes to have compact areas classified as forests without many outliers inside them, she will set the <span class="math inline">\(\sigma^2\)</span> parameter for the class Forest to be high. For comparison, to avoid that small watersheds with few similar neighbors being relabeled, it is advisable to avoid a strong influence of the neighbors, setting <span class="math inline">\(\sigma^2\)</span> to be as low as possible.</p>
</div>
<div id="running-bayesian-smoothing" class="section level2 unnumbered">
<h2>Running Bayesian smoothing<a class="anchor" aria-label="anchor" href="#running-bayesian-smoothing"><i class="fas fa-link"></i></a>
</h2>
<p>In the example below, we create a probability cube based on an existing local file.</p>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define the classes of the probability cube</span></span>
<span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"1"</span> <span class="op">=</span> <span class="st">"Water"</span>,</span>
<span>  <span class="st">"2"</span> <span class="op">=</span> <span class="st">"Clear_Cut_Burned_Area"</span>,</span>
<span>  <span class="st">"3"</span> <span class="op">=</span> <span class="st">"Clear_Cut_Bare_Soil"</span>,</span>
<span>  <span class="st">"4"</span> <span class="op">=</span> <span class="st">"Clear_Cut_Vegetation"</span>,</span>
<span>  <span class="st">"5"</span> <span class="op">=</span> <span class="st">"Forest"</span>,</span>
<span>  <span class="st">"6"</span> <span class="op">=</span> <span class="st">"Wetland"</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># directory where the data is stored</span></span>
<span><span class="va">data_dir</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.file.html">system.file</a></span><span class="op">(</span><span class="st">"extdata/Rondonia-20LLQ/"</span>, package <span class="op">=</span> <span class="st">"sitsdata"</span><span class="op">)</span></span>
<span><span class="co"># create a probability data cube from a file</span></span>
<span><span class="va">probs_cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_cube.html">sits_cube</a></span><span class="op">(</span></span>
<span>  source <span class="op">=</span> <span class="st">"MPC"</span>,</span>
<span>  collection <span class="op">=</span> <span class="st">"SENTINEL-2-L2A"</span>,</span>
<span>  data_dir <span class="op">=</span> <span class="va">data_dir</span>,</span>
<span>  bands <span class="op">=</span> <span class="st">"probs"</span>,</span>
<span>  labels <span class="op">=</span> <span class="va">labels</span>,</span>
<span>  parse_info <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"satellite"</span>, <span class="st">"sensor"</span>, <span class="st">"tile"</span>,</span>
<span>    <span class="st">"start_date"</span>, <span class="st">"end_date"</span>, <span class="st">"band"</span>, <span class="st">"version"</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># plot the probabilities for water and forest</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">probs_cube</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Water"</span>, <span class="st">"Forest"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-142"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-142-1.png" alt="Probability map produced for classes Forest and Water (Source: Authors)." width="100%"><p class="caption">
Figure 73: Probability map produced for classes Forest and Water (Source: Authors).
</p>
</div>
<p>The probability map for class Forest shows high values associated with compact patches and linear stretches in riparian areas. By contrast, the probability map for class Water has mostly low values, except in a few places with a high chance of occurrence of this class. To further understand the behavior of the Bayesian estimator, it is helpful to examine the local variance associated with the logits of the probabilities.</p>
<p>Our first reference is the classified map without smoothing, which shows the presence of outliers and classification errors. To obtain it, we use <code><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification()</a></code>, taking the probability map as an input, as follows.</p>
<div class="sourceCode" id="cb153"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate the thematic map</span></span>
<span><span class="va">class_map</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"no_smooth"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot the result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">class_map</span>,</span>
<span>  tmap_options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"scale"</span> <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-143"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-143-1.png" alt="Classified map without smoothing (Source: Authors)." width="100%"><p class="caption">
Figure 74: Classified map without smoothing (Source: Authors).
</p>
</div>
<p>To remove the outliers and classification errors, we run a smoothing procedure with <code><a href="https://rdrr.io/pkg/sits/man/sits_smooth.html">sits_smooth()</a></code> with parameters: (a) <code>cube</code>, a probability cube produced by <code><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify()</a></code>; (b) <code>window_size</code>, the local window to compute the neighborhood probabilities; (d) <code>neigh_fraction</code>, fraction of local neighbors used to calculate local statistics; (e) <code>smoothness</code>, a vector with estimates of the prior variance of each class; (f) <code>multicores</code>, number of CPU cores that will be used for processing; (g) <code>memsize</code>, memory available for classification; (h) <code>output_dir</code>, a directory where results will be stored; (i) <code>version</code>, for version control. The resulting cube can be visualized with <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code>. In what follows, we compare the smoothing effect by varying the <code>window_size</code> and <code>smoothness</code> parameters.</p>
<p>Together, the parameters <code>window_size</code> and <code>neigh_fraction</code> control how many pixels in a neighborhood the Bayesian estimator will use to calculate the local statistics. For example, setting <code>window size</code> to <span class="math inline">\(7\)</span> and <code>neigh_fraction</code> to <span class="math inline">\(0.50\)</span> (the defaults) ensures that <span class="math inline">\(25\)</span> samples are used to estimate the local statistics.</p>
<p>The <code>smoothness</code> values for all classes aare set to the same value of <span class="math inline">\(20\)</span>, which is relatively high. In this case, for most situations, the new value of the probability will be strongly influenced by the local average.</p>
<div class="sourceCode" id="cb154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute Bayesian smoothing</span></span>
<span><span class="va">cube_smooth_w7_f05_s20</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_smooth.html">sits_smooth</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  window_size <span class="op">=</span> <span class="fl">7</span>,</span>
<span>  neigh_fraction <span class="op">=</span> <span class="fl">0.50</span>,</span>
<span>  smoothness <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-s20"</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot the result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cube_smooth_w7_f05_s20</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Water"</span>, <span class="st">"Forest"</span><span class="op">)</span>, palette <span class="op">=</span> <span class="st">"YlGn"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-144"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-144-1.png" alt="Probability maps after bayesian smoothing (Source: Authors)." width="100%"><p class="caption">
Figure 75: Probability maps after bayesian smoothing (Source: Authors).
</p>
</div>
<p>Bayesian smoothing has removed some of the local variability associated with misclassified pixels that differ from their neighbors. There is a side effect: the water areas surrounded by forests have not been preserved in the forest probability map. The smoothing impact is best appreciated by comparing the labeled map produced without smoothing to the one that follows the procedure, as shown below.</p>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate the thematic map</span></span>
<span><span class="va">defor_map_w7_f05_20</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">cube_smooth_w7_f05_s20</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp8"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-s20"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">defor_map_w7_f05_20</span>,</span>
<span>  tmap_options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"scale"</span> <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-145"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-145-1.png" alt="Final classification map after Bayesian smoothing with 7 x 7 window, using neigh_fraction = 0.5 and smoothness = 20 (Source: Authors)." width="100%"><p class="caption">
Figure 76: Final classification map after Bayesian smoothing with 7 x 7 window, using neigh_fraction = 0.5 and smoothness = 20 (Source: Authors).
</p>
</div>
<p>In the smoothed map, the outliers have been removed by expanding forest areas. Forests have replaced small corridors of water and soil encircled by trees. This effect is due to the high probability of forest detection in the training data. To keep the water areas and reduce the expansion of the forest area, a viable alternative is to reduce the smoothness (<span class="math inline">\(\sigma^2\)</span>) for Forest and Water classes. In this way, the local influence of the forest in the other classes is reduced. As for the water areas, since they are narrow, their neighborhoods will have many low probability values, which would reduce the expected value of the Bayesian estimator.</p>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Reduce smoothing for classes Water and Forest</span></span>
<span><span class="co"># Labels:  "Water", "ClearCut_Burn", "ClearCut_Soil",</span></span>
<span><span class="co">#          "ClearCut_Veg", "Forest", "Wetland"</span></span>
<span><span class="va">smooth_water_forest</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">20</span>, <span class="fl">20</span>, <span class="fl">20</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span></span>
<span><span class="co"># Compute Bayesian smoothing</span></span>
<span><span class="va">cube_smooth_w7_f05_swf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_smooth.html">sits_smooth</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  window_size <span class="op">=</span> <span class="fl">7</span>,</span>
<span>  neigh_fraction <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  smoothness <span class="op">=</span> <span class="va">smooth_water_forest</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-swf"</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Computed labeled map</span></span>
<span><span class="va">defor_map_w7_f05_swf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">cube_smooth_w7_f05_swf</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-swf"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">defor_map_w7_f05_swf</span>,</span>
<span>  tmap_options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"scale"</span> <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-146"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-146-1.png" alt="Probability maps after Bayesian smoothing with 7 x 7 window with low smoothness for classes Water and Forest (Source: Authors)." width="100%"><p class="caption">
Figure 77: Probability maps after Bayesian smoothing with 7 x 7 window with low smoothness for classes Water and Forest (Source: Authors).
</p>
</div>
<p>Comparing the two maps, the narrow water streams inside the forest area have been better preserved. Small corridors between forest areas have also been maintained. For A better comparison between the two maps requires importing them into QGIS.</p>
<p>In conclusion, post-processing is a desirable step in any classification process. Bayesian smoothing improves the borders between the objects created by the classification and removes outliers that result from pixel-based classification. It is a reliable method that should be used in most situations.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="image-classification-in-data-cubes.html">Image classification in data cubes</a></div>
<div class="next"><a href="validation-and-accuracy-measurements.html">Validation and accuracy measurements</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-smoothing-for-post-processing">Bayesian smoothing for post-processing</a></li>
<li><a class="nav-link" href="#introduction-1">Introduction</a></li>
<li><a class="nav-link" href="#empirical-bayesian-estimation">Empirical Bayesian estimation</a></li>
<li><a class="nav-link" href="#using-non-isotropic-neighbourhoods">Using non-isotropic neighbourhoods</a></li>
<li><a class="nav-link" href="#effect-of-the-hyperparameter">Effect of the hyperparameter</a></li>
<li><a class="nav-link" href="#running-bayesian-smoothing">Running Bayesian smoothing</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</strong>" was written by Gilberto Camara, Rolf Simoes, Felipe Souza, Charlotte Pelletier, Alber Sanchez, Pedro R. Andrade, Karine Ferreira, Gilberto Queiroz. It was last built on 2023-11-06.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
