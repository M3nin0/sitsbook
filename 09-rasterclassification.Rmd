```{r, include = FALSE}
source("common.R")
dir.create("./tempdir/chp9")
sits:::.conf_set_options("tmap_legend_text_size" = 0.5)
sits:::.conf_set_options("tmap_legend_title_size" = 0.5)
```

# Image classification in data cubes{-}


<a href="https://www.kaggle.com/esensing/raster-classification-in-sits" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

This Chapter discusses how to classify data cubes by providing a step-by-step example. Our study area is the state of Rondonia, Brazil, which underwent substantial deforestation in the last decades. The objective of the case study is to detect deforested areas. 

## Building a data cube{-}

The case study uses a data cube with Sentinel-2 images available in the package `sitsdata`. These images are from the `SENTINEL-2-L2A` collection in Microsoft Planetary Computer (`MPC`). We have chosen bands BO2, B8A, and B11, and indexes NDVI, EVI and NBR in a small area of $1200 \times 1200$ pixels in the state of Rondonia. As explained in Chapter [Earth observation data cubes](https://e-sensing.github.io/sitsbook/earth-observation-data-cubes.html), we must inform `sits` how to parse these file names to obtain tile, date, and band information. Image files are named according to the convention "satellite_ sensor_tile_band_date" (e.g., `SENTINEL-2_MSI_20LKP_BO2_2020_06_04.tif`) which is the default format in `sits`. 


```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Color composite image of the cube for date 2021-07-25 (Source: Authors)."}
# Files are available in a local directory 
data_dir <- system.file("extdata/Rondonia-20LMR/", package = "sitsdata")
# Read data cube
ro_cube_20LMR <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c("satellite", "sensor", "tile", "band", "date")
)

# Plot the cube
plot(ro_cube_20LMR, date = "2022-07-16", red = "B11", green = "B8A", blue = "B02")
```


## Training the classification model{-}

The case study uses the training data set `samples_deforestation_rondonia`, available in package `sitsdata`. This data set consists of 6007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: "Clear_Cut_Bare_Soil", "Clear_Cut_Burned_Area", "Mountainside_Forest", "Forest", "Riparian_Forest", "Clear_Cut_Vegetation", "Water", "Wetland", and "Seasonally_Flooded". Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B07, B8A, B08, B11, B12 and indices NDVI, EVI and NBR  from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have been collected by remote sensing experts using visual interpretation. In what follows, we will use a subset of the data, consisting of bands B02,  B8A and B11, and indexes NDVI, EVI, and NBR. The reason for this choice is because we will classify a subset of a data cube covering the state of Rondonia which has these bands and 
indexes.

```{r, tidy = "styler"}
library(sitsdata)
# Obtain the samples 
data("samples_deforestation_rondonia")
# Select the bands and indexes used in the example
samples_deforestation <- sits_select(
    samples_deforestation_rondonia,
    bands = c("B02", "B8A", "B11", "NDVI", "EVI", "NBR")
)
# Show the contents of the samples
# 
summary(samples_deforestation)
```

It is helpful to plot the basic patterns associated with the samples to understand the training set better. The function `sits_patterns()` uses a generalized additive model (GAM) to predict a smooth, idealized approximation to the time series associated with each class for all bands. Since the data cube used in the classification has three bands (B02, B8A, and B11) and three indexes (NDVI, EVI, NBR), we filter the samples for these bands and indexes before showing the patterns. 

```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Patterns associated to the training samples (Source: Authors)."}
samples_deforestation |> 
    sits_select(bands = c("NDVI", "EVI", "NBR")) |> 
    sits_patterns() |> 
    plot()
```

The patterns show different temporal responses for the selected classes. They match the typical behavior of deforestation in the Amazon. In most cases, the forest is cut at the start of the dry season (May/June). At the end of the dry season, some clear-cut areas are burned to clean the remains; this action is reflected in the steep fall of the response of NBR values of burned area samples after August. The areas where native trees have been cut but some vegatation remain ("Clear_Cut_Vegetation") have values in the NBR band that increase during the period. This is a sign of mixed pixels, which combine forest remains with bare soil. 

## Model tuning{-}

Model tuning is the process of selecting the best set of hyperparameters for a specific application. When using deep learning models for image classification, it is a highly recommended step to enable a better fit of the algorithm to the training data. Hyperparameters are parameters of the model that are not learned during training but instead are set prior to training and affect the behavior of the model during training. Examples include the learning rate, batch size, number of epochs, number of hidden layers, number of neurons in each layer, activation functions, regularization parameters, and optimization algorithms.

Deep learning model tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model on a given task. This is done by training and evaluating the model with different sets of hyperparameters to select the set that gives the best performance.

Deep learning algorithms try to find the optimal point representing the best value of the prediction function that, given an input $X$ of data points, predicts the result $Y$. In our case, $X$ is a multidimensional time series, and $Y$ is a vector of probabilities for the possible output classes. For complex situations, the best prediction function is time-consuming to estimate. For this reason, deep learning methods rely on gradient descent methods to speed up predictions and converge faster than an exhaustive search [@Bengio2012]. All gradient descent methods use an optimization algorithm adjusted with hyperparameters such as the learning and regularization rates [@Schmidt2021]. The learning rate controls the numerical step of the gradient descent function, and the regularization rate controls model overfitting. Adjusting these values to an optimal setting requires using model tuning methods. 

To reduce the learning curve, `sits` provides default values for all machine learning and deep learning methods, ensuring a reasonable baseline performance. However, refininig model hyperparameters might be necessary, especially for more complex models such as `sits_lighttae()` or `sits_tempcnn()`. To that end, the package provides the `sits_tuning()` function. 

The most straightforward approach to model tuning is to run a grid search; this involves defining a range for each hyperparameter and then testing all possible combinations. This approach leads to a combinational explosion and thus is not recommended. Instead, Bergstra and Bengio propose randomly chosen trials [@Bergstra2012]. Their paper shows that randomized trials are more efficient than grid search trials, selecting adequate hyperparameters at a fraction of the computational cost. The `sits_tuning()` function follows Bergstra and Bengio by using a random search on the chosen hyperparameters.

Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research [@Bottou2018]. Many optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. [@Schmidt2021]. The Adamw optimizer  provides a good baseline and reliable performance for general deep learning applications [@Kingma2017]. 

Experiments with image time series show that other optimizers may have better performance for the specific problem of land classification. For this reason, the authors developed the  `torchopt` R package, which includes several recently proposed optimizers, including Madgrad [@Defazio2021], and Yogi [@Zaheer2018]. Using the `sits_tuning()` function allows testing these and other optimizers available in `torch` and `torch_opt` packages.  

The `sits_tuning()` function takes the following parameters:

1. `samples`: Training data set to be used by the model.
2. `samples_validation`: Optional data set containing time series to be used for validation. If missing, the next parameter will be used.
3. `validation_split`: If `samples_validation` is not used, this parameter defines the proportion of time series in the training data set to be used for validation (default is 20%).
4. `ml_method()`: Deep learning method (either `sits_mlp()`, `sits_tempcnn()`, `sits_resnet()`, `sits_tae()` or `sits_lighttae()`).
5. `params`: Defines the optimizer and its hyperparameters by calling  `sits_tuning_hparams()`, as shown in the example below. 
6. `trials`: Number of trials to run the random search.
7. `multicores`: Number of cores to be used for the procedure.
8. `progress`: Show a progress bar?

The `sits_tuning_hparams()` function inside `sits_tuning()` allows defining optimizers and their hyperparameters, including `lr` (learning rate), `eps` (controls numerical stability), and `weight_decay` (controls overfitting). The default values for `eps` and `weight_decay` in all `sits` deep learning functions are 1e-08  and 1e-06, respectively. The default `lr` for `sits_lighttae()` and `sits_tempcnn()` is  0.005, and for `sits_tae()` and `sits_resnet()` is 0.001. Users have different ways to randomize the hyperparameters, including: `choice()` (a list of options), `uniform` (a uniform distribution), `randint` (random integers from a uniform distribution), `normal(mean, sd)` (normal distribution), and `beta(shape1, shape2)` (beta distribution) and `loguniform(max, min)` (loguniform distribution). These options allow an extensive combination of hyperparameters.

We suggest to use the log-uniform distribution to search over a wide range of values that span several orders of magnitude. This is common for hyperparameters like learning rates, which can vary from very small values (e.g., 0.0001) to larger values (e.g., 1.0) in a logarithmic manner. By default, `sits_tuning()` uses a loguniform distribution between 10^-2 and 10^-4 for the learning rate and the same distribution between 10^-2 and 10^-8 for the weight decay.

In the example, we use `sits_tuning()` with the training data set `samples_deforestation_rondonia`, available in package `sitsdata`. This data set consists of 6007 samples collected from Sentinel-2 images covering the state of Rondonia. More details on this data set are available in Chapter [Image classification in data cubes](https://e-sensing.github.io/sitsbook/image-classification-in-data-cubes.html). The function finds good hyperparameters to train `sits_lighttae()` for the training data set. It tests 20 combinations of learning rate and weight decay for the Adamw optimizer. 

```{r, tidy="styler", eval = FALSE, echo = TRUE}
tuned <- sits_tuning(
     samples = samples_deforestation_rondonia,
     ml_method = sits_lighttae(),
     params = sits_tuning_hparams(
         optimizer = torchopt::optim_adamw,
         opt_hparams = list(
             lr = loguniform(10^-2, 10^-4),
             weight_decay = loguniform(10^-2, 10^-8)
         ),
     ),
     trials = 20,
     multicores = 6,
     progress = FALSE
)
```

```{r, eval = TRUE, echo = FALSE}

tuned <- readRDS("./etc/tuned.rds")
```

The result is a tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The 10 best results obtain accuracy values between 0.919 and 0.908, as shown below. The best result is obtained by a learning rate of 2e-04 and a weight decay of 6.3e-06.

```{r} 
# Obtain accuracy, kappa, lr, and weight decay for the five best results
# Hyperparameters are organized as a list
hparams_5 <- tuned[1:5,]$opt_hparams
# Extract learning rate and weight decay from the list
lr_5 <- purrr::map_dbl(hparams_5, function(h) h$lr)
wd_5 <- purrr::map_dbl(hparams_5, function(h) h$weight_decay)

# Create a tibble to display the results
best_5 <- tibble::tibble(
    accuracy = tuned[1:5,]$accuracy,
    kappa = tuned[1:5,]$kappa,
    lr    = lr_5,
    weight_decay = wd_5)
# Print the five best combination of hyperparameters
best_5
```
To understand the need for tuning, consider the worst 5 results, as show below. The worst result in the 20 trials is an accuracy of 0.829, when the learning rate is set to 5.6e-04 and the weight_decay to 2.57e-07

```{r} 
# Obtain accuracy, kappa, lr, and weight decay for the five best results
# Hyperparameters are organized as a list
hparams_5_worst <- tuned[16:20,]$opt_hparams
# Extract learning rate and weight decay from the list
lr_5_worst <- purrr::map_dbl(hparams_5, function(h) h$lr)
wd_5_worst <- purrr::map_dbl(hparams_5, function(h) h$weight_decay)

# Create a tibble to display the results
worst_5 <- tibble::tibble(
    accuracy = tuned[16:20, ]$accuracy,
    kappa = tuned[16:20, ]$kappa,
    lr    = lr_5_worst,
    weight_decay = wd_5_worst)
# Print the five best combination of hyperparameters
worst_5
```


For large data sets, the tuning process is time-consuming. Despite this cost, it is recommended to achieve the best performance. In general, tuning hyperparameters for models such as `sits_tempcnn()` and `sits_lighttae()` will result in a slight performance improvement over the default parameters on overall accuracy. The performance gain will be stronger in the less well represented classes, where significant gains in producer's and user's accuracies are possible. When detecting change in less frequent classes, tuning can make a substantial difference in the results.



## Training a machine learning model{-}

The next step is to train a random forest model. Since the data cube to be classified has bands BO2, B8A, and B11, we use the selection made in the previous step. 

```{r, tidy = "styler", out.width = "80%", fig.align="center", fig.cap="Relevant attributes of the random forest model (Source: Authors)."}
# Train model using Random Fores
rfor_model <- sits_train(
    samples = samples_6bands, 
    ml_method = sits_rfor()
)

# Plot the model
plot(rfor_model)
```

## Classification using parallel processing{-}

To classify both data cubes and sets of time series, use `sits_classify()`, which uses parallel processing to speed up the performance, as described at the end of this Chapter. Its most relevant parameters are: (a) `data`, either a data cube or a set of time series; (b) `ml_model`, a trained model using one of the machine learning methods provided; (c) `multicores`, number of CPU cores that will be used for processing; (d) `memsize`, memory available for classification; (e) `output_dir`, directory where results will be stored; (f) `version`, for version control. To follow the processing steps, turn on the parameters `verbose` to print information and `progress` to get a progress bar. The classification result is a data cube with a set of probability layers, one for each output class. Each probability layer contains the model's assessment of how likely each pixel belongs to the related class. The probability cube can be visualized with `plot()`. 

```{r, tidy = "styler", out.width = "80%", fig.align="center", fig.cap="Probability maps produced by random forest model (Source: authors)."}

# Classify data cube to obtain a probability cube
ro_cube_20LMR_probs <- sits_classify(
    data     = ro_cube_20LMR,
    ml_model = rfor_model,
    output_dir = "./tempdir/chp9",
    version = "rfor_6bands",
    multicores = 4,
    memsize = 12)

plot(ro_cube_20LMR_probs, palette = "YlGn")
```

The probability cube provides information on the output values of the algorithm for each class. In deep learning algorithms, the last step before producing the output is to apply a neural network layer just before the output layer, which assigns probabilities to each class in a multi-class problem. Those probabilities add up to 1.0. Machine learning algorithms such as random forests also produce a probability cube. 

Most probability maps contain outliers or misclassified pixels. The labeled map generated from the pixel-based time series classification method exhibits several misclassified pixels, which are small patches surrounded by a different class. This occurrence of outliers is a common issue that arises due to the inherent nature of this classification approach. Regardless of their resolution, mixed pixels are prevalent in images, and each class exhibits considerable data variability. As a result, these factors can lead to outliers that are more likely to be misclassified. To overcome this limitation, `sits` employs post-processing smoothing techniques that leverage the spatial context of the probability cubes to refine the results. These techniques will be discussed in the Chapter [Bayesian smoothing for post-processing](https://e-sensing.github.io/sitsbook/bayesian-smoothing-for-post-processing.html). 

Thus, in general, users should perform a post-processing smoothing after obtaining the probability maps. To further illustrate such need, let us consider the case when the labelling operation is performed directly after the classification. To do so, we use the function `sits_label_classification()`, as shown below. 

```{r, tidy = "styler", out.width = "120%", fig.align="center", fig.cap="Final classification map (Source: Authors)."}
# Generate a thematic map
defor_map <- sits_label_classification(
    cube = ro_cube_20LMR_probs,
    clean = TRUE,
    window_size = 3L,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp9",
    version = "no_smooth")

plot(defor_map, tmap_options = 
         list("tmap_legend_text_size" = 0.6,
              "tmap_legend_title_size" = 0.6))
```

For each pixel, the `sits_label_classification()` function takes the label with highest probability and assigns it to the resulting map. The output is a labelled map with classes. In addition, the function will remove of outliers after the labelling step if the `clean` parameter is set to `TRUE` (which is the default value). In this case, for each pixel of the labelled map, it scans a window defined by the `window_size` parameter (default = 3) and assigns such pixel to the most frequent label inside the window.

## Map reclassification{-}

Reclassification of a remote sensing map refers to changing the classes assigned to different pixels in the image. The purpose of reclassification is to modify the information contained in the image to better suit a specific use case. In `sits`, reclassification involves assigning new classes to pixels based on additional information from a reference map. Users define rules according to the desired outcome. These rules are then applied to the classified map. The result is a new map with updated classes.

To illustrate the reclassification in `sits`, we take a classified data cube stored in the `sitsdata` package. As discussed in Chapter [Earth observation data cubes](https://e-sensing.github.io/sitsbook/earth-observation-data-cubes.html), `sits` can create a data cube from a classified image file. Users need to provide the original data source and collection, the directory where data is stored (`data_dir`), the information on how to retrieve data cube parameters from file names (`parse_info`), and the labels used in the classification. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Original classification map (Source: Authors)."}
# Open classification map
data_dir <- system.file("extdata/Rondonia-Class", package = "sitsdata")
ro_class <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c("satellite", "sensor", "tile", "start_date", "end_date",
                   "band", "version"),
    bands = "class",
    labels = c("1" = "Water", "2" = "ClearCut_Burn", "3" = "ClearCut_Soil",
               "4" = "ClearCut_Veg", "5" = "Forest", 
               "6" = "Bare_Soil", "7" = "Wetland"))

plot(ro_class)
```

The above map shows the total extent of deforestation by clear cuts estimated by the `sits` random forest algorithm in an area in Rondonia, Brazil, based on a time series of Sentinel-2 images for the period 2020-06-04 to 2021-08-26. Suppose we want to estimate the deforestation that occurred from June 2020 to August 2021. We need a reference map containing information on forest cuts before 2020. 

In this example, we use as a reference the PRODES deforestation map of Amazonia created by Brazil's National Institute for Space Research (INPE). This map is produced by visual interpretation. PRODES measures deforestation every year, starting from August of one year to July of the following year. It contains classes that represent the natural world (Forest, Water, NonForest, and  NonForest2) and classes that capture the yearly deforestation increments. These classes are named "dYYYY" and "rYYYY"; the first refers to deforestation in a given year (e.g., "d2008" for deforestation for August 2007 to July 2008); the second to places where the satellite data is not sufficient to determine the land class (e.g., "r2010" for 2010). This map is available on package `sitsdata`, as shown below.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Deforestation map produced by sits (Source: Authors)."}
data_dir <- system.file("extdata/PRODES", package = "sitsdata")
prodes2021 <- sits_cube(
    source = "USGS",
    collection = "LANDSAT-C2L2-SR",
    data_dir = data_dir,
    parse_info = c("product", "sensor", "tile", "start_date", "end_date",
                   "band", "version"),
    bands = "class",
    version = "v20220606",
    labels = c("1" = "Forest", "2" = "Water", "3" = "NonForest",
               "4" = "NonForest2", "6" = "d2007", "7" = "d2008",
               "8" = "d2009", "9" = "d2010", "10" = "d2011", 
               "11" = "d2012", "12" = "d2013", "13" = "d2014", 
               "14" = "d2015", "15" = "d2016", "16" = "d2017",
               "17" = "d2018", "18" = "r2010", "19" = "r2011",
               "20" = "r2012", "21" = "r2013", "22" = "r2014", 
               "23" = "r2015", "24" = "r2016", "25" = "r2017", 
               "26" = "r2018", "27" = "d2019", "28" = "r2019", 
               "29" = "d2020", "31" = "r2020", "32" = "Clouds2021",
               "33" = "d2021", "34" = "r2021")
    )
```

Since the labels of the deforestation map are specialized and are not part of the default `sits` color table, we define a legend for better visualization of the different deforestation classes. 

```{r, tidy = "styler", echo = TRUE, eval = FALSE}
# Use the RColorBrewer palette "YlOrBr" for the deforestation years
colors <- grDevices::hcl.colors(n = 15, palette = "YlOrBr")
# Define the legend for the deforestation map
def_legend <- c(
    "Forest" = "forestgreen", "Water" = "dodgerblue3", 
    "NonForest" = "bisque2", "NonForest2" = "bisque2",
    "d2007" = colors[1],  "d2008" = colors[2],
    "d2009" = colors[3],  "d2010" = colors[4], 
    "d2011" = colors[5],  "d2012" = colors[6],
    "d2013" = colors[7],  "d2014" = colors[8],
    "d2015" = colors[9],  "d2016" = colors[10],
    "d2017" = colors[11], "d2018" = colors[12],
    "d2019" = colors[13], "d2020" = colors[14], 
    "d2021" = colors[15], "r2010" = "lightcyan",
    "r2011" = "lightcyan", "r2012"= "lightcyan" 
    "r2013" = "lightcyan", "r2014" = "lightcyan", 
    "r2015" = "lightcyan", "r2016" = "lightcyan", 
    "r2017" = "lightcyan", "r2018" = "lightcyan", 
    "r2019" = "lightcyan", "r2020" = "lightcyan",
    "r2021" = "lightcyan", "Clouds2021" = "lightblue2")
```

Using this new legend, we can visualize the PRODES deforestation map.
```{r, echo = TRUE, eval = FALSE}
sits_view(prodes2021, legend = def_legend)
```

```{r prodes, echo = FALSE, out.width="90%", fig.align="center", fig.cap="Deforestation map produced by PRODES."}
knitr::include_graphics("images/view_prodes_2021.png")
```

Taking the PRODES map as our reference, we can include new labels in the classified map produced by `sits` using `sits_reclassify()`. The new class name Defor_2020 will be applied to all pixels that PRODES considers that have been deforested before July 2020. We also include a Non_Forest class to include all pixels that PRODES takes as not covered by native vegetation, such as wetlands and rocky areas. The PRODES classes will be used as a mask over the `sits` deforestation map.

The `sits_reclassify()` operation requires the parameters: (a) `cube`, the classified data cube whose pixels will be reclassified; (b) `mask`, the reference data cube used as a mask; (c) `rules`, a named list. The names of the `rules` list will be the new label. Each new label is associated with a `mask` vector that includes the labels of the reference map that will be joined. `sits_reclassify()` then compares the original and reference map pixel by pixel. For each pixel of the reference map whose labels are in one of the `rules`, the algorithm relabels the original map. The result will be a reclassified map with the original labels plus the new labels that have been masked using the reference map.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Deforestation map by sits masked by PRODES map (Source: Authors)."}
# Reclassify cube
ro_def_2021 <- sits_reclassify(
    cube = ro_class,
    mask = prodes2021,
    rules = list(
        "Non_Forest" = mask %in% c("NonForest", "NonForest2"),
        "Deforestation_Mask" = mask %in% c(
            "d2007", "d2008", "d2009",
            "d2010", "d2011", "d2012",
            "d2013", "d2014", "d2015",
            "d2016", "d2017", "d2018",
            "d2019", "d2020",
            "r2010", "r2011", "r2012",
            "r2013", "r2014", "r2015",
            "r2016", "r2017", "r2018",
            "r2019", "r2020", "r2021"),
        "Water" = mask == "Water"),
    memsize = 8,
    multicores = 2,
    output_dir = "./tempdir/chp9",
    version = "reclass")

# Plot the reclassified map
plot(ro_def_2021)
```

The reclassified map has been split into deforestation before mid-2020 (using the PRODES map) and the areas classified by `sits` that are taken as being deforested from mid-2020 to mid-2021. This allows the experts to measure how much deforestation occurred in this period according to `sits` and compare the result with the PRODES map. 

The `sits_reclassify()` function is not restricted to comparing deforestation maps. It can be used in any case that requires masking of a result based on a reference map. 

